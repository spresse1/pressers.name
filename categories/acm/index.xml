<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ACM on The Electronic Press</title><link>https://pressers.name/categories/acm/</link><description>Recent content in ACM on The Electronic Press</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 17 Dec 2012 05:25:42 +0000</lastBuildDate><atom:link href="https://pressers.name/categories/acm/index.xml" rel="self" type="application/rss+xml"/><item><title>Building the ACM Cluster, Part 11g: OpenAFS RPM Build</title><link>https://pressers.name/2012/12/17/building-acm-cluster-part-11f-openafs-rpm-build/</link><pubDate>Mon, 17 Dec 2012 05:25:42 +0000</pubDate><guid>https://pressers.name/2012/12/17/building-acm-cluster-part-11f-openafs-rpm-build/</guid><description>OpenAFS is the open source version of the AFS - a file system developed at Carnegie-Mellon University. AFS has a global, DNS-based address space. It also has a ton of nice features with respect to allowing users to create and control their own groups and much more granular permissions. All in all it seems to be a good way to get data into a cluster and to allow users to store and manage documents in a reliable format.</description></item><item><title>Building the ACM Cluster, part 11f: ZFS on Linux</title><link>https://pressers.name/2012/12/16/building-acm-cluster-part-11f-zfs-linux/</link><pubDate>Sun, 16 Dec 2012 23:59:22 +0000</pubDate><guid>https://pressers.name/2012/12/16/building-acm-cluster-part-11f-zfs-linux/</guid><description>ZFS is one of those pieces of software that is almost frighteningly good at what it does. It has a whole slew of features that make it, for many uses, the perfect filesystem. These include: deduplication, compression, data integrity guarantees (ZFS can detect and repair silent data corruption), copy-on-write architecture and a built in concept of RAID. The only problem is that the source is under a license incompatible with the linux kernel, so it will never be kernel mainline.</description></item><item><title>Building the ACM Cluster, Part 11e: Building Ceph</title><link>https://pressers.name/2012/12/16/building-acm-cluster-part-11e-building-ceph/</link><pubDate>Sun, 16 Dec 2012 16:32:07 +0000</pubDate><guid>https://pressers.name/2012/12/16/building-acm-cluster-part-11e-building-ceph/</guid><description>Ceph is a distributed storage engine. It can be used in a whole number of different ways - for example, as a block device or an object store. The current version is codenamed argonaut, hence the header image.
In the ACM cluster, we&amp;rsquo;re using it as the storage engine for VMs. This makes a lot of sense in our case, as the VMs are going to want to move from machine to machine and this stops them having to copy the disk image.</description></item><item><title>Building the ACM Cluster, Part 11d: Building Myricom fiber RPMs</title><link>https://pressers.name/2012/12/16/building-myricom-fiber-rpms/</link><pubDate>Sun, 16 Dec 2012 05:49:26 +0000</pubDate><guid>https://pressers.name/2012/12/16/building-myricom-fiber-rpms/</guid><description>Welcome back to my ongoing series on building the JHUACM VM cluster. In this part, I&amp;rsquo;m going to be focusing on building the RPM driver for the Myricom fiber cards that were given to us with the cluster. Unfortunately the drivers for this are closed source. However, through my connections to physics, I was able to get source code to build from. In short, if you&amp;rsquo;re here looking for drivers, you&amp;rsquo;re out of luck - go talk to Myricom.</description></item><item><title>Building the ACM Cluster, Part 11c: Xen RPMs</title><link>https://pressers.name/2012/12/16/building-acm-cluster-part-11c-xen-rpms/</link><pubDate>Sun, 16 Dec 2012 04:24:34 +0000</pubDate><guid>https://pressers.name/2012/12/16/building-acm-cluster-part-11c-xen-rpms/</guid><description>Next up, lets build RPMs of Xen, a hypervisor. Xen was chosen because on machines which do not have virtualization bits (like the cluster I&amp;rsquo;m building), Xen will do paravirtualization, which is still somewhat quick.Xen also has the concept of clustering and shifting VMs between instances - an important feature in a VM cluster!
Xen Spec Files CentOS 6 no longer has support for Xen. CentOS decided that they were going to put their weight behind QEMU/KVM as the virtulization solution and thus stopped distributing and supporting Xen.</description></item><item><title>Building the ACM Cluster, Part 11b: Kernel RPM Build</title><link>https://pressers.name/2012/12/16/building-acm-cluster-part-11b-kernel-rpm-build/</link><pubDate>Sun, 16 Dec 2012 04:11:20 +0000</pubDate><guid>https://pressers.name/2012/12/16/building-acm-cluster-part-11b-kernel-rpm-build/</guid><description>If you haven&amp;rsquo;t already done so, read and execute Building the ACM Cluster, Part 11a: Setting up rpmbuild environment.
This article will be covering building a new kernel for CentOS and injecting it into xCat&amp;rsquo;s local package repository. I am covering this because later on we&amp;rsquo;ll need to have a more recent kernel than CentOS comes with by default. Xen specifically requires a later kernel. However, when we build kernel modules, we&amp;rsquo;ll want to be building against the same kernel version we&amp;rsquo;re running.</description></item><item><title>Building the ACM Cluster, Part 11a: Setting up rpmbuild environment</title><link>https://pressers.name/2012/12/15/building-acm-cluster-part-11a-setting-rpmbuild-environment/</link><pubDate>Sat, 15 Dec 2012 20:14:28 +0000</pubDate><guid>https://pressers.name/2012/12/15/building-acm-cluster-part-11a-setting-rpmbuild-environment/</guid><description>Up to this point, we haven&amp;rsquo;t built any custom software for the cluster. I&amp;rsquo;ve tried very hard to use mostly off the shelf software. However, this has to change. Several of the major components we&amp;rsquo;re going to use (xen, the fiber card driver, ceph) are not available in the CentOS repositories (or are too old). So we&amp;rsquo;re going to build them ourselves.
However, rather than build them on a node-by node basis (which would make a single install hours long&amp;hellip;), we&amp;rsquo;re going to build packages.</description></item><item><title>Building the ACM VM Cluster, Part 10: Operating system image build</title><link>https://pressers.name/2012/12/15/building-acm-vm-cluster-part-8-operating-system-image-build/</link><pubDate>Sat, 15 Dec 2012 18:40:42 +0000</pubDate><guid>https://pressers.name/2012/12/15/building-acm-vm-cluster-part-8-operating-system-image-build/</guid><description>Now that we&amp;rsquo;re done with network configuration. Now, lets actually build an operating system to use on the nodes!
Lets go ISO Huntin'! The first step in building operating system install images is to get the full operating system images. Not netboot, but a fully installable version. For CentOS the mirrors page is a good place to start your hunt. Personally, I downloaded both DVD images (CentOS-6.3-x86_64-bin-DVD1.iso and CentOS-6.3-x86_64-bin-DVD2.iso), though I suspect that simply the minimal image will cover it.</description></item><item><title>Building the ACM Cluster, Part 9: Setting up masquerade with iptables</title><link>https://pressers.name/2012/12/15/building-acm-cluster-part-8-setting-masquerade-iptables/</link><pubDate>Sat, 15 Dec 2012 18:22:19 +0000</pubDate><guid>https://pressers.name/2012/12/15/building-acm-cluster-part-8-setting-masquerade-iptables/</guid><description>Alright! Lets get this started again. There is one last thing we need to do in order to have networking on the cluster functional. Right now, the nodes inside the cluster can&amp;rsquo;t speak to the outside world. While we set up the head node to be able to speak to things on every interface, we haven&amp;rsquo;t yet told it how to move traffic from one interface to another.
Making the Gateway In normal clusters, there are three types of notes - workers, gateways and head nodes.</description></item><item><title>ACM Talk: Version Control</title><link>https://pressers.name/2012/10/26/acm-talk-version-control/</link><pubDate>Fri, 26 Oct 2012 02:59:56 +0000</pubDate><guid>https://pressers.name/2012/10/26/acm-talk-version-control/</guid><description>I gave a talk to JHUACM this evening about version control. Pretty basic stuff. Anyway, I thought I&amp;rsquo;d put a clearer link to my slides online in case it was useful for anyone.
Slides, PDF foramt
Slides, ODP format</description></item><item><title>Building the ACM Cluster, Part 8: Adventures in Routing: Source Based (Multi-homed) Routing</title><link>https://pressers.name/2012/10/17/adventures-routing-source-based-multi-homed-routing/</link><pubDate>Wed, 17 Oct 2012 12:29:56 +0000</pubDate><guid>https://pressers.name/2012/10/17/adventures-routing-source-based-multi-homed-routing/</guid><description>(This post is related to the ACM cluster build. However, it is really generic systems stuff and not terribly related to the actual cluster build. It is much more closely related to quirks of JHU networking.)
The Problem JHU has two distinct networks - firewalled and firewall-free. (In truth there are more and there are gradations, but these are the two JHUACM has IP allocations on.) Some services cannot be run form inside the firewalled network.</description></item><item><title>Building the ACM Cluster, Part 7: Network redux</title><link>https://pressers.name/2012/10/11/building-acm-cluster-part-7-network-redux/</link><pubDate>Thu, 11 Oct 2012 01:11:47 +0000</pubDate><guid>https://pressers.name/2012/10/11/building-acm-cluster-part-7-network-redux/</guid><description>So I&amp;rsquo;ve mentioned that I&amp;rsquo;ve been fighting networking again in the ACM cluster. I&amp;rsquo;ve been reworking the network. This whole adventure began after a conversation with the very knowledgeable nwf, who pointed out that JHU runs two different networks that the ACM systems need access to - CSNet (the JHU Computer Science department&amp;rsquo;s network) and JHU firewall-free (which has unfiltered access to the internet). The goal of this rework was to allow the cluster to be on both.</description></item><item><title>Configuring Cisco 2900XL Switches via TFTP (and alongside xCat)</title><link>https://pressers.name/2012/09/11/configuring-cisco-2900xl-switches-tftp-and-alongside-xcat/</link><pubDate>Tue, 11 Sep 2012 01:36:59 +0000</pubDate><guid>https://pressers.name/2012/09/11/configuring-cisco-2900xl-switches-tftp-and-alongside-xcat/</guid><description>As part of my ACM cluster creation obsession, I wanted to make the Cisco 2900XL switches we have autoconfigure. I&amp;rsquo;m not terribly familiar with how these switches operate and, rather than reconfigure all of them at once, I&amp;rsquo;d much rather be able to reboot (reload, in cisco parlence) them and have them come up with my new, clean config.
Fortunately for me, Cisco thought of a situation like this when building the software for these switches.</description></item><item><title>Building the ACM Cluster, Part 6: Network Configuration</title><link>https://pressers.name/2012/09/09/building-acm-cluster-part-6-network-configuration/</link><pubDate>Sun, 09 Sep 2012 21:51:04 +0000</pubDate><guid>https://pressers.name/2012/09/09/building-acm-cluster-part-6-network-configuration/</guid><description>In part 5 of this series, I covered the generic xCat setup and we told it how the network was set up. We&amp;rsquo;ve done little bits of network setup before (like setting our switches up for SNMP read access in the public community). Now we&amp;rsquo;re going to do quite a bit more.
Telling xCat where Nodes Are xCat has this wonderful ability to know what a newly plugged in machine is, based on where it is on a switch.</description></item><item><title>Building the ACM VM Cluster, Part 5: xCat configuration</title><link>https://pressers.name/2012/09/09/building-acm-vm-cluster-part-5-xcat-configuration/</link><pubDate>Sun, 09 Sep 2012 19:41:05 +0000</pubDate><guid>https://pressers.name/2012/09/09/building-acm-vm-cluster-part-5-xcat-configuration/</guid><description>In part 4 of this series, we installed xCat. Now its time to configure it.
But First&amp;hellip; But first, lets talk a little about how you interact with xCat. xCat stores its configuration files in &amp;ldquo;tab&amp;quot;s - tables. In the default case, these are sqlite tables, though some will try to convince you to change it to MySQL. Unless you&amp;rsquo;re running thousands of nodes or continually hitting xCat, I see no point in this changeover and therefore will not cover it.</description></item><item><title>Building the ACM VM Cluster, Part 4: xCat Install</title><link>https://pressers.name/2012/09/09/building-acm-vm-cluster-part-4-xcat-install/</link><pubDate>Sun, 09 Sep 2012 13:19:43 +0000</pubDate><guid>https://pressers.name/2012/09/09/building-acm-vm-cluster-part-4-xcat-install/</guid><description>In part 3 of this series, we got the management node set up to route traffic and generally properly configured. Now its time to actually install xCat!
Choices Many xCat guides recommend downloading the RPMs and manually installing those. The benefit of this route is that you need no network connection to your management node for this setup. However, I prefer to have a simple upgrade path. I also know my cluster will continuously be connected to the internet, so I don&amp;rsquo;t need an offline configuration.</description></item><item><title>Building the ACM VM Cluster, Part 3: Mangement Node Setup - Keepalived</title><link>https://pressers.name/2012/09/08/building-acm-vm-cluster-part-3-keepalived/</link><pubDate>Sat, 08 Sep 2012 21:45:31 +0000</pubDate><guid>https://pressers.name/2012/09/08/building-acm-vm-cluster-part-3-keepalived/</guid><description>In part 2 of this series, I covered the network design - the last theoretical piece of design we need. Now let&amp;rsquo;s do some practical stuff! In this section I&amp;rsquo;m going to cover base network setup and keepalived installation on the management node.
Prerequisites For this I&amp;rsquo;ve assumed you already have a set up CentOS machine. In my case, its CentOS 6.3 (64-bit) though this may of course vary, in which case, refer to the XCat documentation to see any differences.</description></item><item><title>Building the ACM VM Cluster, Part 2: Network design</title><link>https://pressers.name/2012/09/08/building-acm-vm-cluster-part-2-network-design/</link><pubDate>Sat, 08 Sep 2012 21:04:35 +0000</pubDate><guid>https://pressers.name/2012/09/08/building-acm-vm-cluster-part-2-network-design/</guid><description>Welcome to part 2 of my series of posts on building the ACM VM cluster! Part 1 covered the hardware and software that will be used in the cluster. Part 2 is going to focus on network design.
Introduction Typically in clusters there are two networks (usually represented by IP ranges, though sometimes actually physically separate): management and work. Management is typically mostly for master node to worker node communication (giving and getting work orders), whereas work is for intercommunication between nodes.</description></item><item><title>Building the ACM VM Cluster, Part 1: Hardware</title><link>https://pressers.name/2012/09/08/building-acm-vm-cluster-part-1/</link><pubDate>Sat, 08 Sep 2012 19:48:49 +0000</pubDate><guid>https://pressers.name/2012/09/08/building-acm-vm-cluster-part-1/</guid><description>So I have a vision for the ACM systems - I&amp;rsquo;d like to make it so we are as redundant as possible and we can do updates to systems without having to take them offline. The obvious answer to this is using VMs, in a cluster. Fortunately, the ACM recently received a donation of the old DLMS cluster from physics (as well as three more recent Dell servers from JHU Housing and Dining).</description></item><item><title>Luna: A Case-Study in Failure</title><link>https://pressers.name/2012/09/08/luna-case-study-failure/</link><pubDate>Sat, 08 Sep 2012 17:23:13 +0000</pubDate><guid>https://pressers.name/2012/09/08/luna-case-study-failure/</guid><description>I intended to make the first post here be about the technology under the site and how to set up a similar site. I will still write that one, but in the meantime a much more interesting problem has come up and I wish to share it with you.
This post is about the various issues, both human and machine that lead to the failure of Luna, the JHU ACM&amp;rsquo;s Xen virtual machine server.</description></item></channel></rss>